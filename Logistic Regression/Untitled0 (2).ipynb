{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0 (2).ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IIhJtogxOXLb"},"source":["**Understanding Logistic Regression in Python**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LicTL2EROR0T","outputId":"cd56fe4a-eb5f-4ca4-8647-f869f556811f","executionInfo":{"status":"ok","timestamp":1653070222349,"user_tz":-330,"elapsed":29484,"user":{"displayName":"Priyanshu Singh","userId":"15297907067997755050"}}},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qu-rDUzrN6hL","outputId":"242773c0-67fa-4f6c-b963-49572d0d2e72","executionInfo":{"status":"ok","timestamp":1653070226947,"user_tz":-330,"elapsed":406,"user":{"displayName":"Priyanshu Singh","userId":"15297907067997755050"}}},"source":["cd /content/gdrive/My Drive/Colab Notebooks/Logistic Regression/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/Colab Notebooks/Logistic Regression\n"]}]},{"cell_type":"code","metadata":{"id":"IswzF7pdNy2O"},"source":["import pandas as pd\n","col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n","# load dataset\n","pima = pd.read_csv(\"pima-indians-diabetes.csv\", header=None, names=col_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"DrK9gqteOGtv","outputId":"3a685e17-b766-450a-a2f2-b4f456b4495a","executionInfo":{"status":"ok","timestamp":1653070247144,"user_tz":-330,"elapsed":9,"user":{"displayName":"Priyanshu Singh","userId":"15297907067997755050"}}},"source":["pima.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   pregnant  glucose  bp  skin  insulin   bmi  pedigree  age  label\n","0         6      148  72    35        0  33.6     0.627   50      1\n","1         1       85  66    29        0  26.6     0.351   31      0\n","2         8      183  64     0        0  23.3     0.672   32      1\n","3         1       89  66    23       94  28.1     0.167   21      0\n","4         0      137  40    35      168  43.1     2.288   33      1"],"text/html":["\n","  <div id=\"df-23d1b069-5593-4aae-9cbb-dc95981e3f4b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pregnant</th>\n","      <th>glucose</th>\n","      <th>bp</th>\n","      <th>skin</th>\n","      <th>insulin</th>\n","      <th>bmi</th>\n","      <th>pedigree</th>\n","      <th>age</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23d1b069-5593-4aae-9cbb-dc95981e3f4b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-23d1b069-5593-4aae-9cbb-dc95981e3f4b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-23d1b069-5593-4aae-9cbb-dc95981e3f4b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"rGficw4XPxbc"},"source":["**Selecting Feature**\n","Here, you need to divide the given columns into two types of variables dependent(or target variable) and independent variable(or feature variables)."]},{"cell_type":"code","metadata":{"id":"io89TlgfOJfB"},"source":["#split dataset in features and target variable\n","feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n","X = pima[feature_cols] # Features\n","y = pima.label # Target variable"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Jm3i14PP4cs"},"source":["**Splitting Data**\n","To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n","\n","Let's split dataset by using function train_test_split(). You need to pass 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly."]},{"cell_type":"code","metadata":{"id":"WzAghBeIP5rV"},"source":["# split X and y into training and testing sets\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ukQcXyRQKLs"},"source":["Here, the Dataset is broken into two parts in a ratio of 75:25. It means 75% data will be used for model training and 25% for model testing.\n","\n","**Model Development and Prediction**\n","First, import the Logistic Regression module and create a Logistic Regression classifier object using LogisticRegression() function.\n","\n","Then, fit your model on the train set using fit() and perform prediction on the test set using predict()."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SV0QAgxJQL9M","outputId":"a58990df-113c-4192-b826-282888c50ac4","executionInfo":{"status":"ok","timestamp":1653070379018,"user_tz":-330,"elapsed":421,"user":{"displayName":"Priyanshu Singh","userId":"15297907067997755050"}}},"source":["# import the class\n","from sklearn.linear_model import LogisticRegression\n","\n","# instantiate the model (using the default parameters)\n","logreg = LogisticRegression()\n","\n","# fit the model with data\n","logreg.fit(X_train,y_train)\n","\n","#\n","y_pred=logreg.predict(X_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]}]},{"cell_type":"markdown","metadata":{"id":"RLun_7eKQTsy"},"source":["**Model Evaluation using Confusion Matrix**\n","A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExNc8FIQQV0q","outputId":"7c1f464d-aef7-469c-e7bc-c620f8502a49","executionInfo":{"status":"ok","timestamp":1653070395705,"user_tz":-330,"elapsed":450,"user":{"displayName":"Priyanshu Singh","userId":"15297907067997755050"}}},"source":["# import the metrics class\n","from sklearn import metrics\n","cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n","cnf_matrix"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[117,  13],\n","       [ 24,  38]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"ptBiEnbTQaoq"},"source":["Here, you can see the confusion matrix in the form of the array object. The dimension of this matrix is 2*2 because this model is binary classification. You have two classes 0 and 1. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. In the output, 119 and 36 are actual predictions, and 26 and 11 are incorrect predictions.\n","\n","Visualizing Confusion Matrix using Heatmap\n","Let's visualize the results of the model in the form of a confusion matrix using matplotlib and seaborn.\n","\n","Here, you will visualize the confusion matrix using Heatmap."]},{"cell_type":"code","metadata":{"id":"oBicNn3OQGJw"},"source":["# import required modules\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nku99k9jQdU4"},"source":["class_names=[0,1] # name  of classes\n","fig, ax = plt.subplots()\n","tick_marks = np.arange(len(class_names))\n","plt.xticks(tick_marks, class_names)\n","plt.yticks(tick_marks, class_names)\n","# create heatmap\n","sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n","ax.xaxis.set_label_position(\"top\")\n","plt.tight_layout()\n","plt.title('Confusion matrix', y=1.1)\n","plt.ylabel('Actual label')\n","plt.xlabel('Predicted label')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eUQbMuvJQl6U"},"source":["**Confusion Matrix Evaluation Metrics**\n","Let's evaluate the model using model evaluation metrics such as accuracy, precision, and recall."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fV2PT4IcQnhD","outputId":"ca3d8380-8d4c-4fca-cd9d-0b025dbfc9cd","executionInfo":{"status":"ok","timestamp":1648016148576,"user_tz":-330,"elapsed":391,"user":{"displayName":"Priyanshu Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYLbmgBYNMFgNZZn4tjDxlo_qij9Lr6V7hdGdEcA=s64","userId":"15297907067997755050"}}},"source":["print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n","print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n","print(\"Recall:\",metrics.recall_score(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8072916666666666\n","Precision: 0.7450980392156863\n","Recall: 0.6129032258064516\n"]}]},{"cell_type":"markdown","metadata":{"id":"kjIGH6YZQtCD"},"source":["Well, you got a classification rate of 80%, considered as good accuracy.\n","\n","**Precision:** Precision is about being precise, i.e., how accurate your model is. In other words, you can say, when a model makes a prediction, how often it is correct. In your prediction case, when your Logistic Regression model predicted patients are going to suffer from diabetes, that patients have 76% of the time.\n","\n","**Recall:** If there are patients who have diabetes in the test set and your Logistic Regression model can identify it 58% of the time.\n","\n","**ROC Curve** \n","Receiver Operating Characteristic(ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"m4IQq8sQQpsT","outputId":"45e4ae9d-dc23-439b-af0f-e647e3cbc9ae","executionInfo":{"status":"ok","timestamp":1648016152697,"user_tz":-330,"elapsed":409,"user":{"displayName":"Priyanshu Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYLbmgBYNMFgNZZn4tjDxlo_qij9Lr6V7hdGdEcA=s64","userId":"15297907067997755050"}}},"source":["y_pred_proba = logreg.predict_proba(X_test)[::,1]\n","fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n","auc = metrics.roc_auc_score(y_test, y_pred_proba)\n","plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"7S4V8dXuQ3iZ"},"source":["AUC score for the case is 0.86. AUC score 1 represents perfect classifier, and 0.5 represents a worthless classifier.\n","\n","**Advantages**\n","Because of its efficient and straightforward nature, doesn't require high computation power, easy to implement, easily interpretable, used widely by data analyst and scientist. Also, it doesn't require scaling of features. Logistic regression provides a probability score for observations.\n","\n","**Disadvantages **\n","Logistic regression is not able to handle a large number of categorical features/variables. It is vulnerable to overfitting. Also, can't solve the non-linear problem with the logistic regression that is why it requires a transformation of non-linear features. Logistic regression will not perform well with independent variables that are not correlated to the target variable and are very similar or correlated to each other.\n","\n","**Conclusion**\n","In this tutorial, you covered a lot of details about Logistic Regression. You have learned what the logistic regression is, how to build respective models, how to visualize results and some of the theoretical background information. Also, you covered some basic concepts such as the sigmoid function, maximum likelihood, confusion matrix, ROC curve.\n","\n","Hopefully, you can now utilize the Logistic Regression technique to analyze your own datasets. Thanks for reading this tutorial!\n","\n","If you would like to learn more about Logistic Regression, take DataCamp's Foundations of Predictive Analytics in Python (Part 1) course."]}]}